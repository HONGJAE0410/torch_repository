{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import os \n",
    "\n",
    "device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet- 18 층 구현 \n",
    "1. 블록구현\n",
    "2. 데이터 불러오기\n",
    "3. 모델 학습 \n",
    "\n",
    "### resnet 층 1개 구조\n",
    "![resnet](/Users/mac/AIworkspace/torchspace/resnet/image/resnet_st.png)\n",
    "- each convolution + batch normalization + activation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        ## 3x3 필터를 사용\n",
    "        # in_planes : 입력 사진의 필터의 수 \n",
    "        # planes : 출력 사진의 필터의 수 \n",
    "        self.conv1 =  nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1 , bias=False )\n",
    "        self.bin1 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bin2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        # 입력값 그대로 출력하는 경우\n",
    "        self.shortcut = nn.Sequential() # 항등함수인 경우 : 입력값 그대로 출력()\n",
    "        \n",
    "\n",
    "        # 항등함수가 아닌경우 : 차원을 맞춰주기 위해 1x1 conv 사용 , stride = 2인경우\n",
    "        # stride=1이 아니면, 사이즈가 반으로 줄어들어야 하기 때문에 stride=1로 설정하여 conv2d 연산을 수행한다. \n",
    "        if stride != 1 or in_planes != planes :\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1 , stride=stride , bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "    # forward : 잔차 + skip connection\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bin1(self.conv1(x))) # 첫번째 conv -> batch -> relu\n",
    "        out = self.bin2(self.conv2(out))\n",
    "        out += self.shortcut(x) # 첫 입력값을 더함 = skip connection\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 모델 구조\n",
    "![resnet_structure](/Users/mac/AIworkspace/torchspace/resnet/image/resnet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes =10): # 블록의 개수를 설정함 \n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64 # 첫번째 conv layer의 input channel 수\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3 , stride=1, padding = 1 , bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block , 64, num_blocks[0] , stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[0],  stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[0] , stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[0], stride=2)\n",
    "        self.linear = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def _make_layer(self, block, planes, num_blocks , stride):\n",
    "        strides= [stride] + [1] * (num_blocks - 1) # 첫번째 convolution layer에 의해서만 높이와 넓이가 줄어들 수 있도록 만든다. \n",
    "        layers= []\n",
    "        for stride in strides: # [2,1,1,1,1,.....] \n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes  # 이번 층의 출력을 다음 층으로 넘기기 위해 입력 채널을 설정\n",
    "        return nn.Sequential(*layers) # 리스트를 인스턴스로 보냄 \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out) \n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4) # 4x4 커널의 평균을 구하겠다. => 최종 출력이 7x7, stride 4로 설정되었기 때문에 1X1 ? 궁금한점 ? \n",
    "        # 왜 이렇게 했지? F.avg_pool2d(out,7) 하면 되는거 아닌가\n",
    "        out = out.view(out.size(0), -1) # 텐서의 형태를 변환하는 함수 -> -1을 활용하여 자동으로 크기를 맞춤 1x1x512이므로 -> 512 차원으로 변환 \n",
    "        # 1차원으로 평탄화. out.size(0)이 배치사이즈이기 때문\n",
    "        out= self.linear(out)\n",
    "        return out \n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=4)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ResNet18()\n",
    "net = net.to(device) \n",
    "net = torch.nn.DataParallel(net)\n",
    "cudnn.benchmark= True\n",
    "\n",
    "learning_rate= 0.1\n",
    "file_name= 'resnet18_cifar10.pt'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # 손실함수 \n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate , momentum=0.9, weight_decay=0.0002)\n",
    "\n",
    "def train(epoch):\n",
    "    print('\\n[ Train epoch: %d ]' % epoch) # 몇번째 에포크인지 \n",
    "    net.train() # train 시작\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader): # input(이미지 데이터) + 정답 레이블로 구성되어있는 두 개의 텐서\n",
    "        inputs, targets = inputs.to(device), targets.to(device) # gpu 설정 \n",
    "        optimizer.zero_grad() # 이전 단계의 기울기 초기화 \n",
    "\n",
    "        # forward propagation \n",
    "        benign_outputs = net(inputs) # 모델에 데이터를 전달하여 예측값 생성 \n",
    "        loss = criterion(benign_outputs, targets) # 손실함수 계산 \n",
    "        loss.backward() # 역전파 \n",
    "\n",
    "        optimizer.step() # 가중치 업데이트 \n",
    "        train_loss += loss.item() # 누적 손실 계산 \n",
    "        _, predicted = benign_outputs.max(1) # 예측 결과 선택 -> 가장 높은 확률값 기준 \n",
    "\n",
    "        total += targets.size(0) # 정답 개수 \n",
    "        correct += predicted.eq(targets).sum().item() # 총 정답 개수 업데이트 \n",
    "        \n",
    "        # 100번째 batch 마다 중간 결과 출력  \n",
    "        if batch_idx % 100 == 0:   \n",
    "            print('\\nCurrent batch:', str(batch_idx))\n",
    "            # train 정확도 출력 : 정확하게 맞춘 샘플의 개수 합산 / 전체 배치 크기 \n",
    "            print('Current benign train accuracy:', str(predicted.eq(targets).sum().item() / targets.size(0)))\n",
    "            print('Current benign train loss:', loss.item())\n",
    "\n",
    "    # 최종 결과 출력 \n",
    "    print('\\nTotal benign train accuarcy:', 100. * correct / total)\n",
    "    print('Total benign train loss:', train_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    print('\\n[ Test epoch: %d ]' % epoch)\n",
    "    net.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        # 전체 배치 크기 \n",
    "        total += targets.size(0)\n",
    "\n",
    "        # 결과 및 loss \n",
    "        outputs = net(inputs)\n",
    "        loss += criterion(outputs, targets).item()\n",
    "\n",
    "        # 가장 높은 확률을 갖는 것을 예측으로 \n",
    "        _, predicted = outputs.max(1)\n",
    "        # 정확하게 맞춘 샘플의 개수 합산 \n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    print('\\nTest accuarcy:', 100. * correct / total)\n",
    "    print('Test average loss:', loss / total)\n",
    "\n",
    "    state = {\n",
    "        'net': net.state_dict()\n",
    "    }\n",
    "    if not os.path.isdir('checkpoint'):\n",
    "        os.mkdir('checkpoint')\n",
    "    torch.save(state, './checkpoint/' + file_name)\n",
    "    print('Model Saved!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에포크가 올라갈수록 학습률을 더 낮춘다. 세밀하게 조정 \n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = learning_rate\n",
    "    if epoch >= 100:\n",
    "        lr /= 10\n",
    "    if epoch >= 150:\n",
    "        lr /= 10\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ Train epoch: 0 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.046875\n",
      "Current benign train loss: 2.4866790771484375\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.296875\n",
      "Current benign train loss: 2.0104289054870605\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.3125\n",
      "Current benign train loss: 1.7229745388031006\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.4375\n",
      "Current benign train loss: 1.6816315650939941\n",
      "\n",
      "Total benign train accuarcy: 31.224\n",
      "Total benign train loss: 737.9201501607895\n",
      "\n",
      "[ Test epoch: 0 ]\n",
      "\n",
      "Test accuarcy: 37.42\n",
      "Test average loss: 0.017374002707004548\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 1 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.5\n",
      "Current benign train loss: 1.4128056764602661\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.484375\n",
      "Current benign train loss: 1.4949407577514648\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.46875\n",
      "Current benign train loss: 1.4096026420593262\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.6015625\n",
      "Current benign train loss: 1.1446890830993652\n",
      "\n",
      "Total benign train accuarcy: 46.466\n",
      "Total benign train loss: 566.9864866733551\n",
      "\n",
      "[ Test epoch: 1 ]\n",
      "\n",
      "Test accuarcy: 37.41\n",
      "Test average loss: 0.01795435833930969\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 2 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.484375\n",
      "Current benign train loss: 1.279184103012085\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.484375\n",
      "Current benign train loss: 1.405977725982666\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.609375\n",
      "Current benign train loss: 1.0995756387710571\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.6171875\n",
      "Current benign train loss: 1.1162961721420288\n",
      "\n",
      "Total benign train accuarcy: 56.152\n",
      "Total benign train loss: 473.88058239221573\n",
      "\n",
      "[ Test epoch: 2 ]\n",
      "\n",
      "Test accuarcy: 50.99\n",
      "Test average loss: 0.01421661252975464\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 3 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.6171875\n",
      "Current benign train loss: 1.102953314781189\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.59375\n",
      "Current benign train loss: 1.1193066835403442\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.6640625\n",
      "Current benign train loss: 1.0136065483093262\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.6796875\n",
      "Current benign train loss: 0.9444999098777771\n",
      "\n",
      "Total benign train accuarcy: 64.0\n",
      "Total benign train loss: 395.606811106205\n",
      "\n",
      "[ Test epoch: 3 ]\n",
      "\n",
      "Test accuarcy: 60.93\n",
      "Test average loss: 0.011515255117416382\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 4 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.7421875\n",
      "Current benign train loss: 0.751548171043396\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.6953125\n",
      "Current benign train loss: 0.9175482988357544\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.6875\n",
      "Current benign train loss: 0.8926778435707092\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.65625\n",
      "Current benign train loss: 0.9867054224014282\n",
      "\n",
      "Total benign train accuarcy: 68.586\n",
      "Total benign train loss: 347.1731035709381\n",
      "\n",
      "[ Test epoch: 4 ]\n",
      "\n",
      "Test accuarcy: 58.68\n",
      "Test average loss: 0.012804099786281586\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 5 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.7265625\n",
      "Current benign train loss: 0.7335156202316284\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.7578125\n",
      "Current benign train loss: 0.7449657917022705\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.7109375\n",
      "Current benign train loss: 0.8060421943664551\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.6875\n",
      "Current benign train loss: 0.8205687403678894\n",
      "\n",
      "Total benign train accuarcy: 72.55\n",
      "Total benign train loss: 306.0631600022316\n",
      "\n",
      "[ Test epoch: 5 ]\n",
      "\n",
      "Test accuarcy: 60.4\n",
      "Test average loss: 0.011960546362400055\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 6 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.7890625\n",
      "Current benign train loss: 0.7165930271148682\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.75\n",
      "Current benign train loss: 0.6670786142349243\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.7421875\n",
      "Current benign train loss: 0.7260874509811401\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.7109375\n",
      "Current benign train loss: 0.7330938577651978\n",
      "\n",
      "Total benign train accuarcy: 75.99\n",
      "Total benign train loss: 268.9360849261284\n",
      "\n",
      "[ Test epoch: 6 ]\n",
      "\n",
      "Test accuarcy: 71.82\n",
      "Test average loss: 0.008265188628435135\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 7 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.7890625\n",
      "Current benign train loss: 0.5720243453979492\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.7890625\n",
      "Current benign train loss: 0.5349916815757751\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.78125\n",
      "Current benign train loss: 0.5944353342056274\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.8046875\n",
      "Current benign train loss: 0.5276338458061218\n",
      "\n",
      "Total benign train accuarcy: 79.126\n",
      "Total benign train loss: 235.97709801793098\n",
      "\n",
      "[ Test epoch: 7 ]\n",
      "\n",
      "Test accuarcy: 76.54\n",
      "Test average loss: 0.006686268123984337\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 8 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.859375\n",
      "Current benign train loss: 0.5228308439254761\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.7578125\n",
      "Current benign train loss: 0.7612775564193726\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.84375\n",
      "Current benign train loss: 0.4829563498497009\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.8515625\n",
      "Current benign train loss: 0.4250757396221161\n",
      "\n",
      "Total benign train accuarcy: 81.48\n",
      "Total benign train loss: 210.90221241116524\n",
      "\n",
      "[ Test epoch: 8 ]\n",
      "\n",
      "Test accuarcy: 75.99\n",
      "Test average loss: 0.007193323650956154\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 9 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.8671875\n",
      "Current benign train loss: 0.4382939338684082\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.8203125\n",
      "Current benign train loss: 0.41783320903778076\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.859375\n",
      "Current benign train loss: 0.45746365189552307\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.8359375\n",
      "Current benign train loss: 0.42288845777511597\n",
      "\n",
      "Total benign train accuarcy: 82.574\n",
      "Total benign train loss: 195.49708795547485\n",
      "\n",
      "[ Test epoch: 9 ]\n",
      "\n",
      "Test accuarcy: 78.72\n",
      "Test average loss: 0.006377697446942329\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 10 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.859375\n",
      "Current benign train loss: 0.3735329508781433\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.859375\n",
      "Current benign train loss: 0.502442479133606\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.8046875\n",
      "Current benign train loss: 0.6493672132492065\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.8671875\n",
      "Current benign train loss: 0.4333135783672333\n",
      "\n",
      "Total benign train accuarcy: 84.214\n",
      "Total benign train loss: 179.24036234617233\n",
      "\n",
      "[ Test epoch: 10 ]\n",
      "\n",
      "Test accuarcy: 82.66\n",
      "Test average loss: 0.005233285224437714\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 11 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.90625\n",
      "Current benign train loss: 0.3180726170539856\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.8359375\n",
      "Current benign train loss: 0.466159462928772\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.84375\n",
      "Current benign train loss: 0.4486789107322693\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.8125\n",
      "Current benign train loss: 0.44202882051467896\n",
      "\n",
      "Total benign train accuarcy: 85.37\n",
      "Total benign train loss: 165.6603101193905\n",
      "\n",
      "[ Test epoch: 11 ]\n",
      "\n",
      "Test accuarcy: 82.31\n",
      "Test average loss: 0.0051458395794034\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 12 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.921875\n",
      "Current benign train loss: 0.2881389260292053\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.875\n",
      "Current benign train loss: 0.3396347761154175\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.859375\n",
      "Current benign train loss: 0.4064943194389343\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.8203125\n",
      "Current benign train loss: 0.5714644193649292\n",
      "\n",
      "Total benign train accuarcy: 85.962\n",
      "Total benign train loss: 158.36136315762997\n",
      "\n",
      "[ Test epoch: 12 ]\n",
      "\n",
      "Test accuarcy: 81.36\n",
      "Test average loss: 0.005694832220673561\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 13 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.84375\n",
      "Current benign train loss: 0.44866931438446045\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.8203125\n",
      "Current benign train loss: 0.5468172430992126\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.859375\n",
      "Current benign train loss: 0.42277225852012634\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.84375\n",
      "Current benign train loss: 0.4969646632671356\n",
      "\n",
      "Total benign train accuarcy: 86.782\n",
      "Total benign train loss: 148.9148531705141\n",
      "\n",
      "[ Test epoch: 13 ]\n",
      "\n",
      "Test accuarcy: 81.46\n",
      "Test average loss: 0.00541386753320694\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 14 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.8515625\n",
      "Current benign train loss: 0.3808598518371582\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.890625\n",
      "Current benign train loss: 0.3431099057197571\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.890625\n",
      "Current benign train loss: 0.311720609664917\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.90625\n",
      "Current benign train loss: 0.3030884861946106\n",
      "\n",
      "Total benign train accuarcy: 87.528\n",
      "Total benign train loss: 140.50190886855125\n",
      "\n",
      "[ Test epoch: 14 ]\n",
      "\n",
      "Test accuarcy: 80.85\n",
      "Test average loss: 0.0060493385732173915\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 15 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.84375\n",
      "Current benign train loss: 0.4388483762741089\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.8671875\n",
      "Current benign train loss: 0.3314882814884186\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.9140625\n",
      "Current benign train loss: 0.3077772557735443\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.8984375\n",
      "Current benign train loss: 0.24421408772468567\n",
      "\n",
      "Total benign train accuarcy: 87.802\n",
      "Total benign train loss: 138.6187229156494\n",
      "\n",
      "[ Test epoch: 15 ]\n",
      "\n",
      "Test accuarcy: 81.68\n",
      "Test average loss: 0.00543123292028904\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 16 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.9296875\n",
      "Current benign train loss: 0.23426221311092377\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.90625\n",
      "Current benign train loss: 0.2613285183906555\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.8984375\n",
      "Current benign train loss: 0.3032494783401489\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.859375\n",
      "Current benign train loss: 0.36173102259635925\n",
      "\n",
      "Total benign train accuarcy: 88.104\n",
      "Total benign train loss: 133.34673726558685\n",
      "\n",
      "[ Test epoch: 16 ]\n",
      "\n",
      "Test accuarcy: 83.07\n",
      "Test average loss: 0.005067135895788669\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 17 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.890625\n",
      "Current benign train loss: 0.26670727133750916\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.921875\n",
      "Current benign train loss: 0.2828654646873474\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.90625\n",
      "Current benign train loss: 0.30918511748313904\n"
     ]
    }
   ],
   "source": [
    "# for epoch in range(0, 200):\n",
    "for epoch in range(0, 20):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
