{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import os \n",
    "\n",
    "device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet- 18 층 구현 \n",
    "1. 블록구현\n",
    "2. 데이터 불러오기\n",
    "3. 모델 학습 \n",
    "\n",
    "### resnet 층 1개 구조\n",
    "![resnet](/Users/mac/AIworkspace/torchspace/resnet/image/resnet_st.png)\n",
    "- each convolution + batch normalization + activation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        ## 3x3 필터를 사용\n",
    "        # in_planes : 입력 사진의 필터의 수 \n",
    "        # planes : 출력 사진의 필터의 수 \n",
    "        self.conv1 =  nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1 , bias=False )\n",
    "        self.bin1 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bin2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        # 입력값 그대로 출력하는 경우\n",
    "        self.shortcut = nn.Sequential() # 항등함수인 경우 : 입력값 그대로 출력()\n",
    "        \n",
    "\n",
    "        # 항등함수가 아닌경우 : 차원을 맞춰주기 위해 1x1 conv 사용 , stride = 2인경우\n",
    "        # stride=1이 아니면, 사이즈가 반으로 줄어들어야 하기 때문에 stride=1로 설정하여 conv2d 연산을 수행한다. \n",
    "        if stride != 1 or in_planes != planes :\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1 , stride=stride , bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "    # forward : 잔차 + skip connection\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bin1(self.conv1(x))) # 첫번째 conv -> batch -> relu\n",
    "        out = self.bin2(self.conv2(out)) \n",
    "        out += self.shortcut(x) # 첫 입력값을 더함 = skip connection\n",
    "        out = F.relu(out)\n",
    "        return out    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bottleneck\n",
    "- 1x1 필터 : 채널을 줄이거나 키울 때 사용 , 채널의 방향의 선형변환만 수행된다. \n",
    "- 3x3 필터 : 피처맵의 특성을 추출한다. \n",
    "\n",
    "-> 1x1을 활용함으로써 연산량을 줄인다. \n",
    "\n",
    "-> 마지막 1x1에서 채널을 4배로 늘리는 이유? \n",
    "1. 더 풍부한 표현 학습 목적\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bottleneck Block 정의\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4  # 마지막 1x1 Conv에서 4배 확장\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "\n",
    "        # 1x1 Conv (Reduce)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        # 3x3 Conv (Convolve)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        # 1x1 Conv (Expand)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "\n",
    "        # Skip Connection (입출력 차원이 다르면 1x1 Conv 적용)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes * self.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes * self.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * self.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))  # 1x1 Conv\n",
    "        out = F.relu(self.bn2(self.conv2(out)))  # 3x3 Conv\n",
    "        out = self.bn3(self.conv3(out))  # 1x1 Conv\n",
    "        out += self.shortcut(x)  # Skip Connection\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 모델 구조\n",
    "![resnet_structure](/Users/mac/AIworkspace/torchspace/resnet/image/resnet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Resnet18(\n",
       "  (c1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (b1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU()\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bin1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bin2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bin1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bin2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bin1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bin2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bin1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bin2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bin1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bin2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bin1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bin2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bin1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bin2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bin1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bin2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ResNet-18 정의 \n",
    "class Resnet18(nn.Module):\n",
    "    def __init__(self, block, num_blcoks , num_classes =1000):\n",
    "        super(Resnet18, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        # 64개의 7x7 필터를 사용\n",
    "        self.c1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.b1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Residual Blocks, make_layer 없이 구현 \n",
    "        self.layer1 =  nn.Sequential(\n",
    "            block(64, 64),\n",
    "            block(64, 64)\n",
    "        )\n",
    "        self.layer2 =  nn.Sequential(\n",
    "            block(64, 128, stride=2),\n",
    "            block(128, 128)\n",
    "        )\n",
    "        self.layer3 =  nn.Sequential(\n",
    "            block(128, 256, stride=2),\n",
    "            block(256, 256)\n",
    "        )\n",
    "        self.layer4 =  nn.Sequential(\n",
    "            block(256, 512, stride=2),\n",
    "            block(512, 512)\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.b1(self.c1(x)))\n",
    "        out = self.maxpool(out)\n",
    "\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1) # flatten\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "Resnet18(BasicBlock, [2, 2, 2, 2]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet-50 네트워크 정의\n",
    "class Resnet50(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=1000):  # ImageNet 기준 1000개 클래스\n",
    "        super(Resnet50, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        # 첫 번째 Conv (논문 그대로 7x7, stride=2 사용)\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)  # 3×3 Max Pooling 추가\n",
    "\n",
    "        # ResNet 블록들 (논문 구조대로 설정)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)  # 56×56 유지\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2) # 56×56 → 28×28\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2) # 28×28 → 14×14\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2) # 14×14 → 7×7\n",
    "\n",
    "        # Global Average Pooling (GAP) + FC Layer\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # 최종 1x1 Feature Map\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)  # Fully Connected Layer\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)  # 첫 블록만 stride 적용\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion  # 현재 블록의 출력 채널을 다음 블록의 입력 채널로 설정\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))  # 7×7 Conv → ReLU\n",
    "        x = self.maxpool(x)  # 3×3 Max Pooling\n",
    "\n",
    "        x = self.layer1(x)  # 56×56 \n",
    "        x = self.layer2(x)  # 28×28\n",
    "        x = self.layer3(x)  # 14×14\n",
    "        x = self.layer4(x)  # 7×7\n",
    "\n",
    "        x = self.avgpool(x)  # 1×1\n",
    "        x = torch.flatten(x, 1)  # Flatten for FC\n",
    "        x = self.fc(x)  # Fully Connected Layer\n",
    "        return x\n",
    "\n",
    "# ResNet-50 생성 함수\n",
    "def ResNet50():\n",
    "    return Resnet50(Bottleneck, [3, 4, 6, 3])  # Bottleneck 구조 사용\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/torch_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=4)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Resnet18.__init__() missing 2 required positional arguments: 'block' and 'num_blcoks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[43mResnet18\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m net \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mto(device) \n\u001b[1;32m      3\u001b[0m net \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mDataParallel(net)\n",
      "\u001b[0;31mTypeError\u001b[0m: Resnet18.__init__() missing 2 required positional arguments: 'block' and 'num_blcoks'"
     ]
    }
   ],
   "source": [
    "net = Resnet18()\n",
    "net = net.to(device) \n",
    "net = torch.nn.DataParallel(net)\n",
    "cudnn.benchmark= True\n",
    "\n",
    "learning_rate= 0.1\n",
    "file_name= 'resnet18_cifar10.pt'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # 손실함수 \n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate , momentum=0.9, weight_decay=0.0002)\n",
    "\n",
    "def train(epoch):\n",
    "    print('\\n[ Train epoch: %d ]' % epoch) # 몇번째 에포크인지 \n",
    "    net.train() # train 시작\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader): # input(이미지 데이터) + 정답 레이블로 구성되어있는 두 개의 텐서\n",
    "        inputs, targets = inputs.to(device), targets.to(device) # gpu 설정 \n",
    "        optimizer.zero_grad() # 이전 단계의 기울기 초기화 \n",
    "\n",
    "        # forward propagation \n",
    "        benign_outputs = net(inputs) # 모델에 데이터를 전달하여 예측값 생성 \n",
    "        loss = criterion(benign_outputs, targets) # 손실함수 계산 \n",
    "        loss.backward() # 역전파 \n",
    "\n",
    "        optimizer.step() # 가중치 업데이트 \n",
    "        train_loss += loss.item() # 누적 손실 계산 \n",
    "        _, predicted = benign_outputs.max(1) # 예측 결과 선택 -> 가장 높은 확률값 기준 \n",
    "\n",
    "        total += targets.size(0) # 정답 개수 \n",
    "        correct += predicted.eq(targets).sum().item() # 총 정답 개수 업데이트 \n",
    "        \n",
    "        # 100번째 batch 마다 중간 결과 출력  \n",
    "        if batch_idx % 100 == 0:   \n",
    "            print('\\nCurrent batch:', str(batch_idx))\n",
    "            # train 정확도 출력 : 정확하게 맞춘 샘플의 개수 합산 / 전체 배치 크기 \n",
    "            print('Current benign train accuracy:', str(predicted.eq(targets).sum().item() / targets.size(0)))\n",
    "            print('Current benign train loss:', loss.item())\n",
    "\n",
    "    # 최종 결과 출력 \n",
    "    print('\\nTotal benign train accuarcy:', 100. * correct / total)\n",
    "    print('Total benign train loss:', train_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    print('\\n[ Test epoch: %d ]' % epoch)\n",
    "    net.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        # 전체 배치 크기 \n",
    "        total += targets.size(0)\n",
    "\n",
    "        # 결과 및 loss \n",
    "        outputs = net(inputs)\n",
    "        loss += criterion(outputs, targets).item()\n",
    "\n",
    "        # 가장 높은 확률을 갖는 것을 예측으로 \n",
    "        _, predicted = outputs.max(1)\n",
    "        # 정확하게 맞춘 샘플의 개수 합산 \n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    print('\\nTest accuarcy:', 100. * correct / total)\n",
    "    print('Test average loss:', loss / total)\n",
    "\n",
    "    state = {\n",
    "        'net': net.state_dict()\n",
    "    }\n",
    "    if not os.path.isdir('checkpoint'):\n",
    "        os.mkdir('checkpoint')\n",
    "    torch.save(state, './checkpoint/' + file_name)\n",
    "    print('Model Saved!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에포크가 올라갈수록 학습률을 더 낮춘다. 세밀하게 조정 \n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = learning_rate\n",
    "    if epoch >= 100:\n",
    "        lr /= 10\n",
    "    if epoch >= 150:\n",
    "        lr /= 10\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ Train epoch: 0 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.046875\n",
      "Current benign train loss: 2.4866790771484375\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.296875\n",
      "Current benign train loss: 2.0104289054870605\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.3125\n",
      "Current benign train loss: 1.7229745388031006\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.4375\n",
      "Current benign train loss: 1.6816315650939941\n",
      "\n",
      "Total benign train accuarcy: 31.224\n",
      "Total benign train loss: 737.9201501607895\n",
      "\n",
      "[ Test epoch: 0 ]\n",
      "\n",
      "Test accuarcy: 37.42\n",
      "Test average loss: 0.017374002707004548\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 1 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.5\n",
      "Current benign train loss: 1.4128056764602661\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.484375\n",
      "Current benign train loss: 1.4949407577514648\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.46875\n",
      "Current benign train loss: 1.4096026420593262\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.6015625\n",
      "Current benign train loss: 1.1446890830993652\n",
      "\n",
      "Total benign train accuarcy: 46.466\n",
      "Total benign train loss: 566.9864866733551\n",
      "\n",
      "[ Test epoch: 1 ]\n",
      "\n",
      "Test accuarcy: 37.41\n",
      "Test average loss: 0.01795435833930969\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 2 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.484375\n",
      "Current benign train loss: 1.279184103012085\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.484375\n",
      "Current benign train loss: 1.405977725982666\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.609375\n",
      "Current benign train loss: 1.0995756387710571\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.6171875\n",
      "Current benign train loss: 1.1162961721420288\n",
      "\n",
      "Total benign train accuarcy: 56.152\n",
      "Total benign train loss: 473.88058239221573\n",
      "\n",
      "[ Test epoch: 2 ]\n",
      "\n",
      "Test accuarcy: 50.99\n",
      "Test average loss: 0.01421661252975464\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 3 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.6171875\n",
      "Current benign train loss: 1.102953314781189\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.59375\n",
      "Current benign train loss: 1.1193066835403442\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.6640625\n",
      "Current benign train loss: 1.0136065483093262\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.6796875\n",
      "Current benign train loss: 0.9444999098777771\n",
      "\n",
      "Total benign train accuarcy: 64.0\n",
      "Total benign train loss: 395.606811106205\n",
      "\n",
      "[ Test epoch: 3 ]\n",
      "\n",
      "Test accuarcy: 60.93\n",
      "Test average loss: 0.011515255117416382\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 4 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.7421875\n",
      "Current benign train loss: 0.751548171043396\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.6953125\n",
      "Current benign train loss: 0.9175482988357544\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.6875\n",
      "Current benign train loss: 0.8926778435707092\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.65625\n",
      "Current benign train loss: 0.9867054224014282\n",
      "\n",
      "Total benign train accuarcy: 68.586\n",
      "Total benign train loss: 347.1731035709381\n",
      "\n",
      "[ Test epoch: 4 ]\n",
      "\n",
      "Test accuarcy: 58.68\n",
      "Test average loss: 0.012804099786281586\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 5 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.7265625\n",
      "Current benign train loss: 0.7335156202316284\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.7578125\n",
      "Current benign train loss: 0.7449657917022705\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.7109375\n",
      "Current benign train loss: 0.8060421943664551\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.6875\n",
      "Current benign train loss: 0.8205687403678894\n",
      "\n",
      "Total benign train accuarcy: 72.55\n",
      "Total benign train loss: 306.0631600022316\n",
      "\n",
      "[ Test epoch: 5 ]\n",
      "\n",
      "Test accuarcy: 60.4\n",
      "Test average loss: 0.011960546362400055\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 6 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.7890625\n",
      "Current benign train loss: 0.7165930271148682\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.75\n",
      "Current benign train loss: 0.6670786142349243\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.7421875\n",
      "Current benign train loss: 0.7260874509811401\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.7109375\n",
      "Current benign train loss: 0.7330938577651978\n",
      "\n",
      "Total benign train accuarcy: 75.99\n",
      "Total benign train loss: 268.9360849261284\n",
      "\n",
      "[ Test epoch: 6 ]\n",
      "\n",
      "Test accuarcy: 71.82\n",
      "Test average loss: 0.008265188628435135\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 7 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.7890625\n",
      "Current benign train loss: 0.5720243453979492\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.7890625\n",
      "Current benign train loss: 0.5349916815757751\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.78125\n",
      "Current benign train loss: 0.5944353342056274\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.8046875\n",
      "Current benign train loss: 0.5276338458061218\n",
      "\n",
      "Total benign train accuarcy: 79.126\n",
      "Total benign train loss: 235.97709801793098\n",
      "\n",
      "[ Test epoch: 7 ]\n",
      "\n",
      "Test accuarcy: 76.54\n",
      "Test average loss: 0.006686268123984337\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 8 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.859375\n",
      "Current benign train loss: 0.5228308439254761\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.7578125\n",
      "Current benign train loss: 0.7612775564193726\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.84375\n",
      "Current benign train loss: 0.4829563498497009\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.8515625\n",
      "Current benign train loss: 0.4250757396221161\n",
      "\n",
      "Total benign train accuarcy: 81.48\n",
      "Total benign train loss: 210.90221241116524\n",
      "\n",
      "[ Test epoch: 8 ]\n",
      "\n",
      "Test accuarcy: 75.99\n",
      "Test average loss: 0.007193323650956154\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 9 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.8671875\n",
      "Current benign train loss: 0.4382939338684082\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.8203125\n",
      "Current benign train loss: 0.41783320903778076\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.859375\n",
      "Current benign train loss: 0.45746365189552307\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.8359375\n",
      "Current benign train loss: 0.42288845777511597\n",
      "\n",
      "Total benign train accuarcy: 82.574\n",
      "Total benign train loss: 195.49708795547485\n",
      "\n",
      "[ Test epoch: 9 ]\n",
      "\n",
      "Test accuarcy: 78.72\n",
      "Test average loss: 0.006377697446942329\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 10 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.859375\n",
      "Current benign train loss: 0.3735329508781433\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.859375\n",
      "Current benign train loss: 0.502442479133606\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.8046875\n",
      "Current benign train loss: 0.6493672132492065\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.8671875\n",
      "Current benign train loss: 0.4333135783672333\n",
      "\n",
      "Total benign train accuarcy: 84.214\n",
      "Total benign train loss: 179.24036234617233\n",
      "\n",
      "[ Test epoch: 10 ]\n",
      "\n",
      "Test accuarcy: 82.66\n",
      "Test average loss: 0.005233285224437714\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 11 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.90625\n",
      "Current benign train loss: 0.3180726170539856\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.8359375\n",
      "Current benign train loss: 0.466159462928772\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.84375\n",
      "Current benign train loss: 0.4486789107322693\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.8125\n",
      "Current benign train loss: 0.44202882051467896\n",
      "\n",
      "Total benign train accuarcy: 85.37\n",
      "Total benign train loss: 165.6603101193905\n",
      "\n",
      "[ Test epoch: 11 ]\n",
      "\n",
      "Test accuarcy: 82.31\n",
      "Test average loss: 0.0051458395794034\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 12 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.921875\n",
      "Current benign train loss: 0.2881389260292053\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.875\n",
      "Current benign train loss: 0.3396347761154175\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.859375\n",
      "Current benign train loss: 0.4064943194389343\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.8203125\n",
      "Current benign train loss: 0.5714644193649292\n",
      "\n",
      "Total benign train accuarcy: 85.962\n",
      "Total benign train loss: 158.36136315762997\n",
      "\n",
      "[ Test epoch: 12 ]\n",
      "\n",
      "Test accuarcy: 81.36\n",
      "Test average loss: 0.005694832220673561\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 13 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.84375\n",
      "Current benign train loss: 0.44866931438446045\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.8203125\n",
      "Current benign train loss: 0.5468172430992126\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.859375\n",
      "Current benign train loss: 0.42277225852012634\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.84375\n",
      "Current benign train loss: 0.4969646632671356\n",
      "\n",
      "Total benign train accuarcy: 86.782\n",
      "Total benign train loss: 148.9148531705141\n",
      "\n",
      "[ Test epoch: 13 ]\n",
      "\n",
      "Test accuarcy: 81.46\n",
      "Test average loss: 0.00541386753320694\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 14 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.8515625\n",
      "Current benign train loss: 0.3808598518371582\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.890625\n",
      "Current benign train loss: 0.3431099057197571\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.890625\n",
      "Current benign train loss: 0.311720609664917\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.90625\n",
      "Current benign train loss: 0.3030884861946106\n",
      "\n",
      "Total benign train accuarcy: 87.528\n",
      "Total benign train loss: 140.50190886855125\n",
      "\n",
      "[ Test epoch: 14 ]\n",
      "\n",
      "Test accuarcy: 80.85\n",
      "Test average loss: 0.0060493385732173915\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 15 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.84375\n",
      "Current benign train loss: 0.4388483762741089\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.8671875\n",
      "Current benign train loss: 0.3314882814884186\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.9140625\n",
      "Current benign train loss: 0.3077772557735443\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.8984375\n",
      "Current benign train loss: 0.24421408772468567\n",
      "\n",
      "Total benign train accuarcy: 87.802\n",
      "Total benign train loss: 138.6187229156494\n",
      "\n",
      "[ Test epoch: 15 ]\n",
      "\n",
      "Test accuarcy: 81.68\n",
      "Test average loss: 0.00543123292028904\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 16 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.9296875\n",
      "Current benign train loss: 0.23426221311092377\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.90625\n",
      "Current benign train loss: 0.2613285183906555\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.8984375\n",
      "Current benign train loss: 0.3032494783401489\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.859375\n",
      "Current benign train loss: 0.36173102259635925\n",
      "\n",
      "Total benign train accuarcy: 88.104\n",
      "Total benign train loss: 133.34673726558685\n",
      "\n",
      "[ Test epoch: 16 ]\n",
      "\n",
      "Test accuarcy: 83.07\n",
      "Test average loss: 0.005067135895788669\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 17 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.890625\n",
      "Current benign train loss: 0.26670727133750916\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.921875\n",
      "Current benign train loss: 0.2828654646873474\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.90625\n",
      "Current benign train loss: 0.30918511748313904\n"
     ]
    }
   ],
   "source": [
    "# for epoch in range(0, 200):\n",
    "for epoch in range(0, 20):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
