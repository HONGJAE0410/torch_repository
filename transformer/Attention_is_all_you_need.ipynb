{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS 장치가 사용 가능한가? True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# mac gpu 연결\n",
    "print(f\"MPS 장치가 사용 가능한가? {torch.backends.mps.is_available()}\") \n",
    "device=torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source (input): tensor([[13481,    18,     4,  ..., 59513, 59513, 59513],\n",
      "        [  138,     6,     9,  ..., 59513, 59513, 59513],\n",
      "        [   84,   581,  9012,  ..., 59513, 59513, 59513],\n",
      "        ...,\n",
      "        [  233,     4,  1497,  ..., 59513, 59513, 59513],\n",
      "        [ 1414, 10983, 13843,  ..., 59513, 59513, 59513],\n",
      "        [  237,   159, 12181,  ..., 59513, 59513, 59513]])\n",
      "Target (label): tensor([[  156,     3, 10815,  ..., 59513, 59513, 59513],\n",
      "        [   87,     6, 42762,  ..., 59513, 59513, 59513],\n",
      "        [  267,  1624,    66,  ..., 59513, 59513, 59513],\n",
      "        ...,\n",
      "        [   89,     6,    82,  ..., 59513, 59513, 59513],\n",
      "        [17523,    51,    34,  ..., 59513, 59513, 59513],\n",
      "        [12187,  6790,  1672,  ..., 59513, 59513, 59513]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/torch_env/lib/python3.12/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "from data.dataloader import get_dataloader\n",
    "from data.tokenizer import Tokenizer\n",
    "# 토크나이저 초기화\n",
    "tokenizer = Tokenizer(model_name='Helsinki-NLP/opus-mt-en-fr')\n",
    "\n",
    "# 데이터 로드\n",
    "train_dataloader = get_dataloader(\"en_fr_data.tsv\", tokenizer, batch_size=32)\n",
    "\n",
    "# 배치 확인\n",
    "for src, trg in train_dataloader:\n",
    "    print(\"Source (input):\", src)\n",
    "    print(\"Target (label):\", trg)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ! pip show torch\n",
    "# ! pip install torch==2.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/torch_env/lib/python3.12/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/opt/anaconda3/envs/torch_env/lib/python3.12/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/opt/anaconda3/envs/torch_env/lib/python3.12/site-packages/torchtext/datasets/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/opt/anaconda3/envs/torch_env/lib/python3.12/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import Multi30k\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "import os \n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://downloads.tatoeba.org/exports/sentences.csv\"\n",
    "# filename = \"sentences.csv\"\n",
    "\n",
    "\n",
    "# if not os.path.exists(filename):\n",
    "#     print(\"Downloading dataset...\")\n",
    "#     urllib.request.urlretrieve(url, filename)\n",
    "#     print(\"Download completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "\n",
    "# data = pd.read_csv(filename, delimiter='\\t', header=None, names=[\"id\", \"lang\", \"sentence\"])\n",
    "# en_sentences = data[data[\"lang\"] == \"eng\"][\"sentence\"].tolist()\n",
    "# fr_sentences = data[data[\"lang\"] == \"fra\"][\"sentence\"].tolist()\n",
    "\n",
    "\n",
    "# # 영어와 프랑스어 문장 쌍을 만듭니다.\n",
    "# en_fr_pairs = list(zip(en_sentences[:10000], fr_sentences[:10000]))\n",
    "\n",
    "\n",
    "# # 데이터를 TSV 파일로 저장합니다.\n",
    "# with open(\"en_fr_data.tsv\", \"w\") as f:\n",
    "#     f.write(\"src\\ttrg\\n\")\n",
    "#     for en, fr in en_fr_pairs:\n",
    "#         f.write(f\"{en}\\t{fr}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import spacy\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# spacy 모델 다운로드 및 로드\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "spacy_fr = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# 토크나이저 정의\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "def tokenize_fr(text):\n",
    "    return [tok.text for tok in spacy_fr.tokenizer(text)]\n",
    "\n",
    "# 데이터셋 로드 및 토큰화\n",
    "def yield_tokens(data_iter, tokenizer):\n",
    "    for data_sample in data_iter:\n",
    "        yield tokenizer(data_sample)\n",
    "\n",
    "with open(\"en_fr_data.tsv\") as f:\n",
    "    data = [line.strip().split(\"\\t\") for line in f.readlines()[1:]]\n",
    "\n",
    "src_text = [item[0] for item in data]\n",
    "trg_text = [item[1] for item in data]\n",
    "\n",
    "SRC_VOCAB = build_vocab_from_iterator(yield_tokens(src_text, tokenize_en), specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "SRC_VOCAB.set_default_index(SRC_VOCAB[\"<unk>\"])\n",
    "TRG_VOCAB = build_vocab_from_iterator(yield_tokens(trg_text, tokenize_fr), specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "TRG_VOCAB.set_default_index(TRG_VOCAB[\"<unk>\"])\n",
    "\n",
    "# 데이터셋 정의\n",
    "def data_process(src_text, trg_text, src_vocab, trg_vocab, src_tokenizer, trg_tokenizer):\n",
    "    data = []\n",
    "    for (src_line, trg_line) in zip(src_text, trg_text):\n",
    "        src_tensor = torch.tensor(\n",
    "            [src_vocab[\"<bos>\"]] +\n",
    "            [src_vocab[token] for token in src_tokenizer(src_line)] +\n",
    "            [src_vocab[\"<eos>\"]],\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        trg_tensor = torch.tensor(\n",
    "            [trg_vocab[\"<bos>\"]] +\n",
    "            [trg_vocab[token] for token in trg_tokenizer(trg_line)] +\n",
    "            [trg_vocab[\"<eos>\"]],\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        data.append((src_tensor, trg_tensor))\n",
    "    return data\n",
    "\n",
    "train_data = data_process(src_text, trg_text, SRC_VOCAB, TRG_VOCAB, tokenize_en, tokenize_fr)\n",
    "\n",
    "# 데이터 로더 생성\n",
    "BATCH_SIZE = 64\n",
    "PAD_IDX = SRC_VOCAB[\"<pad>\"]\n",
    "\n",
    "def generate_batch(data_batch):\n",
    "    src_batch, trg_batch = [], []\n",
    "    for (src_item, trg_item) in data_batch:\n",
    "        src_batch.append(src_item)\n",
    "        trg_batch.append(trg_item)\n",
    "    src_batch = nn.utils.rnn.pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    trg_batch = nn.utils.rnn.pad_sequence(trg_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, trg_batch\n",
    "\n",
    "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([31, 64]) torch.Size([29, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7594, 8469)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_iter)\n",
    "# 훈련 데이터 출력 \n",
    "for src, trg in train_iter:\n",
    "    print(src.shape, trg.shape)\n",
    "    break\n",
    "len(SRC_VOCAB), len(TRG_VOCAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer 정의\n",
    "## multi-head attention \n",
    "- query\n",
    "- key\n",
    "- value\n",
    "\n",
    "- hyperparameter\n",
    "    - hidden_dim: 하나의 단어에 대한 임베딩 차원\n",
    "    - n_heads : 멀티헤드의 개수\n",
    "    - dropou_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  torch.nn as nn\n",
    "\n",
    "class Multiheadattentionlayer(nn.Module): \n",
    "    def __init__(self , hidden_dim, n_heads, dropout_ratio , device):\n",
    "        super().__init__()\n",
    "\n",
    "        assert hidden_dim % n_heads == 0 \n",
    "\n",
    "        self.hidden_dim = hidden_dim # 한 단어에 대한 임베딩 차원 \n",
    "        self.n_heads = n_heads # 멀티헤드 몇개로 설정할 것인지\n",
    "        self.head_dim = hidden_dim // n_heads #각 헤드에서의 임베딩 차원\n",
    "        self.fc_q = nn.Linear(hidden_dim, hidden_dim) # query에 대한 선형 레이어 : 선형 변환을 해줘야 하기 때문\n",
    "        self.fc_k = nn.Linear(hidden_dim, hidden_dim) # 선형 변환 하는 차원 변환 : hidden_dim -> hidden_dim(동일한 차원)\n",
    "        self.fc_v = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_o = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device) # 헤드 차원으로 나눠줘야하기 때문에\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        Q = self.fc_q(query) # (batch_size, query_len, hidden_dim) : 한 번에 처리할 배치, 문장의 길이, 임베딩 차원\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)    \n",
    "\n",
    "        # Q, K, V를 헤드의 개수로 나눠줌 : head_dim = hidden_dim // n_heads \n",
    "        # 0,2,1,3 : 0번째 차원은 그대로 두고 1번째 차원과 2번째 차원을 바꿔줌 : (batch_size, n_heads, query_len, head_dim)\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
    "\n",
    "        # attention energy 계산 : Q와 K의 내적을 구함 : (batch_size, n_heads, query_len, key_len)\n",
    "        # 내적을 하기 위해 permute를 해줌 \n",
    "        energy = torch.matmul(Q, K.permute(0,1,3,2)) / self.scale\n",
    "\n",
    "        # mask를 적용하여 padding 부분을 -1e10으로 채워주어 softmax 후에 0이 되도록 함\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        attention = torch.softmax(energy, dim=-1) # (batch_size, n_heads, query_len, key_len)\n",
    "\n",
    "        # 여기서 dropout을 적용 후 V와 내적\n",
    "        x= torch.matmul(self.dropout(attention), V) # (batch_size, n_heads, query_len, head_dim)\n",
    "        x= x.permute(0,2,1,3).contiguous() # (batch_size, query_len, n_heads, head_dim) : 다시 원래 차원으로 복구\n",
    "        x= x.view(batch_size, -1, self.hidden_dim) # (batch_size, query_len, hidden_dim) : 다시 원래 차원으로 복구\n",
    "        x=self.fc_o(x) # (batch_size, query_len, hidden_dim) \n",
    "\n",
    "        return x, attention\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Position-wise feedforward\n",
    "- 입력과 출력 차원이 동일 \n",
    "- 하이퍼 파라미터 \n",
    "    - hidden_dim\n",
    "    - pf_dim  : feedforward 레이어에서의 내부 임베딩 차원 \n",
    "    - dropout_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Positionwisefeedforwardlayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, pf_dim, dropout_ratio):\n",
    "        super().__init__()\n",
    "        # 선형 변환 레이어 2개로 구성\n",
    "        self.fc_1 = nn.Linear(hidden_dim, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hidden_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x= self.dropout(torch.relu(self.fc_1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PositionalEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, hidden_dim, max_len, device):\n",
    "        super().__init__()\n",
    "        # positional encoding을 위한 비어있는 tensor 생성\n",
    "        self.device = device\n",
    "        self.pe = torch.zeros(max_len, hidden_dim).to(device)\n",
    "\n",
    "        # positional encoding 계산\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float() # 0부터 max_len까지의 숫자를 생성\n",
    "        div_term = torch.exp(torch.arange(0, hidden_dim, 2).float() * (-torch.log(torch.tensor(10000.0)) / hidden_dim))\n",
    "        # 2로 나눈 이유는 sin과 cos를 번갈아가면서 넣어주기 위함\n",
    "        \n",
    "        self.pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.pe = self.pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.shape[1]]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# encoderlayer\n",
    "- 입력과 출력 차원 동일\n",
    "- 하이퍼파라미터\n",
    "    - hidden_dim\n",
    "    - n_heads\n",
    "    - pf_dim  \n",
    "    - dropout_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoderlayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads, pf_dim, dropout_ratio, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        # attention, 피드포워드 , 정규화, 드롭아웃 층으로 구성\n",
    "        # 순서 \n",
    "        # self attention -> 정규화+residual -> positionwise feedforward -> 정규화+residual\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.self_attention = Multiheadattentionlayer(hidden_dim, n_heads, dropout_ratio, device)\n",
    "        self.positionwise_feedforward = Positionwisefeedforwardlayer(hidden_dim, pf_dim, dropout_ratio)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        # src : (batch_size, src_len, hidden_dim)\n",
    "        # src_mask : (batch_size, src_len) : padding 부분을 0으로 표시한 mask\n",
    "        # 쿼리 키 밸류 모두 src로 들어감\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        \n",
    "        # 정규화, 잔차연결\n",
    "        src = self.self_attn_layer_norm(src + self.dropout(_src)) # dropout, residual, layer norm\n",
    "\n",
    "        # position-wise feedforward\n",
    "        _src = self.positionwise_feedforward(src)\n",
    "\n",
    "        # 정규화 잔차연결\n",
    "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
    "\n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder architectue\n",
    "- 하이퍼 파라미터\n",
    "    - input_dim : 하나의 단어에 대한 원 핫 인코딩 차원 \n",
    "    - hidden_dim : 하나의 단어에 대한 임베딩 차원\n",
    "    - n_layers : 쌓을 인코더의 개수\n",
    "    - n_heads\n",
    "    - pf_dim\n",
    "    - dropout_rate\n",
    "    - max_length : 문장 내 최대 단어 개수\n",
    "- positional embedding\n",
    "    - bert 방식 : 학습하는 형태\n",
    "    - transformer 방식  : cos, sin 함수를 활용하는 형태\n",
    "- pad 토큰에 대해 마스크 값을 0으로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, device, max_length=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        # 토큰 임베딩 : 단어를 임베딩\n",
    "        # 포지션 임베딩 : 단어의 위치를 임베딩\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hidden_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hidden_dim)\n",
    "\n",
    "        # 인코더 레이어를 n_layers 만큼 쌓음\n",
    "        self.layers = nn.ModuleList([Encoderlayer(hidden_dim, n_heads, pf_dim, dropout_ratio, device) for _ in range(n_layers)])\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device) # 헤드 차원으로 나눠줘야함\n",
    "    \n",
    "    # bert식\n",
    "    def forward(self, src, src_mask):\n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1] # 문장의 길이\n",
    "\n",
    "        # 포지션 임베딩 : 0부터 src_len까지의 숫자를 순서대로 나타냄\n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device) # (batch_size, src_len)\n",
    "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos)) # (batch_size, src_len, hidden_dim) -> 임베딩 + 포지션 임베딩\n",
    "\n",
    "        # 모든 인코더 레이어를 차례대로 거침\n",
    "        # src : (batch_size, src_len, hidden_dim)\n",
    "        # src_mask\n",
    "        for layer in self.layers:\n",
    "            src= layer(src, src_mask)\n",
    "        return src\n",
    "\n",
    "    # 실제 포지션 임베딩 적용 \n",
    "    def forward2(self, src, src_mask):\n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "\n",
    "        # 코사인, 사인 함수 사용한 임베딩 추가\n",
    "        pos = PositionalEncoding(src.shape[2], src_len, self.device) # (batch_size, src_len, hidden_dim)\n",
    "        src = self.dropout((self.tok_embedding(src) * self.scale) + pos)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        \n",
    "        return src\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  ..., 97, 98, 99],\n",
       "        [ 0,  1,  2,  ..., 97, 98, 99],\n",
       "        [ 0,  1,  2,  ..., 97, 98, 99],\n",
       "        ...,\n",
       "        [ 0,  1,  2,  ..., 97, 98, 99],\n",
       "        [ 0,  1,  2,  ..., 97, 98, 99],\n",
       "        [ 0,  1,  2,  ..., 97, 98, 99]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, 100).unsqueeze(0).repeat(50, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# decoder layer\n",
    "- 입력과 출력의 차원이 같음 \n",
    "- 두 개의 Multi head attention 사용\n",
    "- 하이퍼 파라미터 \n",
    "    - hidden_dim\n",
    "    - n_heads\n",
    "    - pf_dim\n",
    "    - dropout_ratio\n",
    "- pad 토큰에 대해 마스크 값을 0으로 설정 \n",
    "- 타겟 문장에서 . 각단어는 다음 단어가 무엇인지 알 수 없도록 만들기 위해 마스크 사용 \n",
    "\n",
    "self - multi head -> add+norm -> multi-head cross attention-> add+norm -> feedforward , add + norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads, pf_dim, dropout_ratio, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hidden_dim) # self attention 후 정규화\n",
    "        self.enc_attn_layer_norm = nn.LayerNorm(hidden_dim) # cross attention 후 정규화\n",
    "        self.ff_layer_norm = nn.LayerNorm(hidden_dim) # feedforward 후 정규화\n",
    "        self.self_attention = Multiheadattentionlayer(hidden_dim, n_heads, dropout_ratio, device) # self attention\n",
    "        self.encoder_attention = Multiheadattentionlayer(hidden_dim, n_heads, dropout_ratio, device) # cross attention\n",
    "        self.positionwise_feedforward = Positionwisefeedforwardlayer(hidden_dim, pf_dim, dropout_ratio) # feedforward\n",
    "        self.dropout = nn.Dropout(dropout_ratio) \n",
    "    \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask) # self attention, query, key, value 모두 trg\n",
    "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg)) # self attention 이후 dropout, residual, layer norm\n",
    "        \n",
    "        # enc_src : 인코더의 출력값\n",
    "        # trg : 디코더의 1번째 레이어의 출력값\n",
    "        _trg , attention = self.encoder_attention(trg, enc_src, enc_src, src_mask) # cross attention, query는 trg, key, value는 enc_src\n",
    "        trg= self.enc_attn_layer_norm(trg + self.dropout(_trg)) # cross attention 이후 dropout, residual, layer norm\n",
    "\n",
    "        _trg = self.positionwise_feedforward(trg) # feedforward\n",
    "        trg = self.ff_layer_norm(trg + self.dropout(_trg)) # feedforward 이후 dropout, residual, layer norm\n",
    "\n",
    "        return trg, attention\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder architecture\n",
    "- 전체 디코더 구조 정의\n",
    "- 하이퍼 파라미터\n",
    "    - output_dim \n",
    "    - hidden_dim\n",
    "    - n_layers : 인코더 레이어 개수\n",
    "    - n_heads : 멀티 헤드 개수\n",
    "    - pf_dim \n",
    "    - dropout_ratio\n",
    "    - max_length \n",
    "\n",
    "- 실제는 디코더 반복적으로 넣어야 하지만, 학습 시에는 한 번에 출력 문장 구해 학습 가능\n",
    "- 소스 문장의 pad 토큰에 대해 마스크 값을 0으로 설정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, device, max_length=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(output_dim, hidden_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hidden_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(hidden_dim, n_heads, pf_dim, dropout_ratio, device) for _ in range(n_layers)])\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n",
    "    \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        pos = torch.arange(0,trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device) # \n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos) # (batch_size, trg_len, hidden_dim): 임베딩 + 포지션 임베딩 \n",
    "        # scale 값 곱하는 이유 : 나중에 softmax를 할 때 값이 너무 작아지는 것을 방지하기 위함\n",
    "        # 초기 임베딩에 sqrt(hidden_dim)을 곱해줌으로써 값이 너무 작아지는 것을 방지하여 gradeint flow 안정화\n",
    "\n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "        output = self.fc_out(trg)\n",
    "\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "    \n",
    "    # pad 토큰에 대해 마스크 값을 0으로 설정 \n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=self.device)).bool()\n",
    "\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "\n",
    "        output , attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        return output, attention\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC_VOCAB)\n",
    "OUTPUT_DIM = len(TRG_VOCAB)\n",
    "HIDDEN_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Src_PAD_IDX = SRC_VOCAB[\"<pad>\"]\n",
    "Trg_PAD_IDX = TRG_VOCAB[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SRC_VOCAB[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
